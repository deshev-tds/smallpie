#!/usr/bin/env python3
import os
import sys
import sounddevice as sd
import soundfile as sf
import queue
from pathlib import Path
from openai import OpenAI
import subprocess
import tempfile

# -----------------------
# CONFIG
# -----------------------
TRAITS_FILE = "damyan_traits.txt"
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# 24 MB ‚Äì –±–µ–∑–æ–ø–∞—Å–µ–Ω –ª–∏–º–∏—Ç –∑–∞ STT (–ø–æ —Ä–∞–∑–º–µ—Ä –Ω–∞ —Ñ–∞–π–ª–∞)
MAX_BYTES = 24 * 1024 * 1024

# –ú–∞–∫—Å–∏–º–∞–ª–Ω–∞ –¥—ä–ª–∂–∏–Ω–∞ –Ω–∞ –∞—É–¥–∏–æ –∑–∞ –µ–¥–∏–Ω API call (gpt-4o-transcribe –ª–∏–º–∏—Ç ‚âà 1400s)
MAX_SECONDS_PER_CALL = 1200  # 20 –º–∏–Ω—É—Ç–∏, –ø–æ–¥ –ª–∏–º–∏—Ç–∞


# -----------------------
# RECORD AUDIO UNTIL ENTER
# -----------------------
def record_audio_until_interrupt(filename="meeting.wav", samplerate=16000):
    print("üéô –ó–∞–ø–æ—á–≤–∞–º –∑–∞–ø–∏—Å. –ù–∞—Ç–∏—Å–Ω–∏ ENTER –∑–∞ —Å–ø–∏—Ä–∞–Ω–µ.")

    q = queue.Queue()

    def callback(indata, frames, time_, status):
        if status:
            print(f"‚ö†Ô∏è {status}", file=sys.stderr)
        q.put(indata.copy())

    with sf.SoundFile(filename, mode='w', samplerate=samplerate, channels=1) as f:
        with sd.InputStream(samplerate=samplerate, channels=1, callback=callback):
            input()  # —á–∞–∫–∞ ENTER
            print("‚èπ –°–ø–∏—Ä–∞–º –∑–∞–ø–∏—Å–∞.")
            while not q.empty():
                f.write(q.get())

    print(f"üíæ –ó–∞–ø–∏—Å–∞–Ω–æ –≤ {filename}")
    return filename


# -----------------------
# INTERNAL: FFmpeg conversion WAV -> MP3
# -----------------------
def convert_to_mp3(src_path, bitrate="48k"):
    mp3_path = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3").name
    cmd = [
        "ffmpeg",
        "-y",
        "-i", src_path,
        "-vn",
        "-acodec", "libmp3lame",
        "-b:a", bitrate,
        mp3_path
    ]
    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return mp3_path


# -----------------------
# INTERNAL: Transcribe one MP3 chunk
# -----------------------
def _transcribe_single(mp3_path):
    with open(mp3_path, "rb") as f:
        t = client.audio.transcriptions.create(
            model="gpt-4o-transcribe",
            file=f
        )
    return t.text.strip()


# -----------------------
# TRANSCRIBE VIA OPENAI (WITH CHUNKING, NO PYDUB)
# -----------------------
def transcribe_cloud(audio_path):
    print("üü¶ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–∞–º —á—Ä–µ–∑ OpenAI Whisper API...")

    # --- Step 1: Convert WAV ‚Üí MP3 ---
    print("üéß –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–∞–º WAV ‚Üí MP3...")
    mp3_path = convert_to_mp3(audio_path)

    size = os.path.getsize(mp3_path)
    print(f"üì¶ MP3 size: {size / 1024 / 1024:.2f} MB")

    # --- Step 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞ –ø—Ä–æ–¥—ä–ª–∂–∏—Ç–µ–ª–Ω–æ—Å—Ç ---
    try:
        probe = subprocess.check_output(
            [
                "ffprobe",
                "-v", "error",
                "-show_entries", "format=duration",
                "-of", "default=noprint_wrappers=1:nokey=1",
                mp3_path,
            ]
        ).decode().strip()
        duration_sec = float(probe)
    except Exception as e:
        print(f"‚ö†Ô∏è ffprobe –ø—Ä–æ–±–ª–µ–º ({e}), –ø—Ä–∏–µ–º–∞–º, —á–µ –µ –∫—Ä–∞—Ç–∫–æ –∏ –ø—Ä–∞—â–∞–º –¥–∏—Ä–µ–∫—Ç–Ω–æ.")
        duration_sec = 0.0

    # –ê–∫–æ –µ –ø–æ–¥ –ª–∏–º–∏—Ç–∞ –ø–æ —Ä–∞–∑–º–µ—Ä –ò –ø–æ –≤—Ä–µ–º–µ ‚Üí –µ–¥–∏–Ω call
    if size <= MAX_BYTES and (duration_sec == 0.0 or duration_sec <= MAX_SECONDS_PER_CALL):
        print("‚û°Ô∏è –ü–æ–¥ –ª–∏–º–∏—Ç–∞ –µ, –∏–∑–ø—Ä–∞—â–∞–º –¥–∏—Ä–µ–∫—Ç–Ω–æ.")
        transcript = _transcribe_single(mp3_path)
        os.remove(mp3_path)
        print("üìÑ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ä—Ç –µ –∑–∞—Ä–µ–¥–µ–Ω.")
        return transcript

    # --- Step 3: Chunking via ffmpeg ---
    print("‚úÇÔ∏è –ù–∞–¥ –ª–∏–º–∏—Ç–∞ –µ, —Ä–µ–∂–∞ –Ω–∞ —á–∞—Å—Ç–∏ —Å ffmpeg...")

    if duration_sec == 0.0:
        # –Ω—è–∫–∞–∫—ä–≤ —Å—Ç—Ä–∞–Ω–µ–Ω —Å–ª—É—á–∞–π, –Ω–æ –¥–∞ –Ω–µ —É–º—Ä–µ–º
        duration_sec = MAX_SECONDS_PER_CALL

    chunk_sec = min(10 * 60, MAX_SECONDS_PER_CALL)  # 10 –º–∏–Ω, –Ω–æ –ø–æ–¥ –º–æ–¥–µ–ª–∞ –ª–∏–º–∏—Ç
    parts = []

    start = 0.0
    idx = 1

    while start < duration_sec:
        end = min(start + chunk_sec, duration_sec)

        temp_mp3 = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3").name

        # Cut chunk with ffmpeg
        subprocess.run(
            [
                "ffmpeg",
                "-y",
                "-i", mp3_path,
                "-ss", str(start),
                "-to", str(end),
                "-c", "copy",
                temp_mp3,
            ],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )

        print(f"üîπ Chunk {idx}: {start:.1f}s ‚Üí {end:.1f}s")

        try:
            part_text = _transcribe_single(temp_mp3)
            parts.append(part_text)
        finally:
            os.remove(temp_mp3)

        idx += 1
        start = end

    os.remove(mp3_path)
    print("üìÑ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ä—Ç –µ –∑–∞—Ä–µ–¥–µ–Ω (–º–Ω–æ–≥–æ —á–∞—Å—Ç–∏).")
    return "\n\n".join(parts)


# -----------------------
# GPT ANALYSIS
# -----------------------
def analyze_with_gpt(meeting_name, meeting_topic, participants, transcript):
    print("üß† –ê–Ω–∞–ª–∏–∑–∏—Ä–∞–º —Å GPT-5.1...")

    prompt = f"""
You are an expert meeting analyst.

Given the raw transcript of a meeting (possibly in multiple languages), do the following:

1) Reconstruct the conversation as a clean dialog with inferred speakers:
   - Use labels like "Speaker 1:", "Speaker 2:", etc.
   - Group consecutive sentences by the same speaker into paragraphs.
   - Do NOT alternate speakers blindly; infer turns by meaning.

2) Extract and list:
   - Concrete actions Damyan must take.
   - Concrete actions other participants must take.
   - Dependencies or blocked items (who/what they depend on).
   - Deadlines or time references, if present.

3) Identify:
   - Misalignments in expectations.
   - Risks (technical, process, interpersonal).

Rules:
- Base everything ONLY on the transcript content.
- If something is implied but not explicit, mark it as "inferred".
- Output must be in English, even if the transcript is not.

Meeting name: {meeting_name}
Topic: {meeting_topic}
Participants (count or description): {participants}

--- TRANSCRIPT START ---
{transcript}
--- TRANSCRIPT END ---
"""

    response = client.responses.create(
        model="gpt-5.1",
        input=prompt,
    )

    return response.output_text


# -----------------------
# SAVE RESULTS
# -----------------------
def save_output(meeting_name, transcript, analysis):
    safe = meeting_name.replace(" ", "_").replace(":", "_")
    folder = Path(f"meeting_{safe}")
    folder.mkdir(exist_ok=True)

    with open(folder / "transcript.txt", "w", encoding="utf-8") as f:
        f.write(transcript)

    with open(folder / "analysis.txt", "w", encoding="utf-8") as f:
        f.write(analysis)

    print(f"üíæ –ó–∞–ø–∏—Å–∞—Ö —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ –≤ {folder}/")


# -----------------------
# UPDATE TRAITS
# -----------------------
def update_traits(transcript, analysis):
    print("üîç –û–±–Ω–æ–≤—è–≤–∞–º —Ñ–∞–π–ª —Å –ª–∏—á–Ω–∏ traits...")

    trait_prompt = f"""
You are building a long-term behavioral profile of Damyan as a collaborator.

Based ONLY on this transcript and analysis:
- Extract Damyan‚Äôs typical communication style.
- Preferred level of structure and clarity.
- Leadership and management tendencies.
- How he handles conflict, underperformance, and uncertainty.
- Any recurring patterns that future AI assistants should know when working with him.

Write 5‚Äì10 bullet points.
No repetition, no flattery, no armchair diagnosis.
Be specific and practical.

--- TRANSCRIPT ---
{transcript}

--- ANALYSIS ---
{analysis}
"""

    resp = client.responses.create(
        model="gpt-5.1",
        input=trait_prompt,
    )

    traits = resp.output_text.strip()

    with open(TRAITS_FILE, "a", encoding="utf-8") as f:
        f.write("\n\n==== NEW SESSION ====\n")
        f.write(traits)

    print("‚úî Trait engine updated.")


# -----------------------
# MAIN
# -----------------------
def main():
    print("üßæ Meeting Assistant v2.0 ‚Äì Cloud Whisper + GPT-5.1")

    meeting_name = input("üìù Meeting name?\n> ")
    meeting_topic = input("üìù Topic?\n> ")
    participants = input("üë• Participants count / description?\n> ")

    print("\nüìå Choose mode:")
    print("1) üéô Record new audio")
    print("2) üìÅ Use existing WAV file")
    mode = input("> ").strip()

    if mode == "1":
        print("‚ñ∂ Press ENTER to start recording...")
        input()
        audio_path = record_audio_until_interrupt()
    else:
        audio_path = input("üìÅ WAV file path:\n> ").strip()
        if not Path(audio_path).exists():
            print("‚ùå File not found!")
            return

    # TRANSCRIBE
    transcript = transcribe_cloud(audio_path)

    # ANALYZE
    analysis = analyze_with_gpt(meeting_name, meeting_topic, participants, transcript)

    # SAVE
    save_output(meeting_name, transcript, analysis)

    # TRAITS
    update_traits(transcript, analysis)


if __name__ == "__main__":
    main()